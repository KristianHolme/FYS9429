---
title: "Towards fast and flexible deep reinforcement learning in Julia"
author: "Kristian Holme"
date: 2025-06-06
format: pdf
    # html:
    #     embed-resources: false

editor: 
    render-on-save: true
csl: ieee.csl
filters:
  - pseudocode
bibliography: references.bib
---

# Abstract

Deep Reinforcement Learning (DRL) is a powerful optimization tool for control tasks, with applications ranging from robotics to active flow control. As part of my PhD research in DRL for flow control, I aim to deepen my understanding of DRL algorithms while laying the groundwork for a fully-featured DRL package in the Julia programming language [@julia:a]. This project implements a prototype DRL library called DRiL.jl[^1] [@holme2025], focusing on the Proximal Policy Optimization (PPO) algorithm. Due to time constraints, the current implementation is limited in scope but successfully demonstrates learning on the classical Pendulum and MountainCar environments. The prototype includes support for vectorized environments, proper implementation of key algorithmic details, and flexible neural network architectures. This work represents the initial foundation for continued development of a comprehensive DRL package in Julia.

[^1]: <https://github.com/kristianholme/DRiL.jl>

# Introduction

Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. At its core, RL involves three fundamental components: environments that define the problem space and dynamics, agents that learn and make decisions, and algorithms that guide the learning process.

Reinforcement learning environments can range from simple abstract problems to complex real-world systems. The classic Grid-World environment, where an agent navigates a discrete grid to collect rewards, serves as a fundamental testbed for developing and understanding RL algorithms. While simple, it captures the essential elements of sequential decision-making and reward maximization.

More complex environments include robotic control tasks, where agents learn to manipulate physical systems either in simulation or through real-world sensors and actuators [@raffin2022]. An emerging area of research applies RL to fluid dynamics, where the goal might be to minimize drag [@suárez]or control circulation patterns in Rayleigh-Bénard convection [@vasanth2024].

In classical reinforcement learning, problems were typically solved using tabular methods where the state and action spaces were discrete and small enough to be represented in lookup tables. Algorithms like Q-learning [@watkins1989] and SARSA [@sutton2018] could store explicit value estimates for each state-action pair, making optimal policies tractable for simple environments like grid worlds, where the player moves on a finite grid, trying to reach grid cells with a high reward and avoid grid cells with a low or negative reward.

However, as RL expanded to tackle real-world problems with high-dimensional observation spaces—such as raw pixel inputs from images or continuous sensor readings—classical tabular approaches became computationally infeasible. This limitation gave rise to modern Deep Reinforcement Learning (DRL), which leverages neural networks as function approximators to handle complex, high-dimensional inputs.

In DRL, observations from the environment are passed through neural networks to produce actions, enabling agents to operate in previously intractable domains. This approach combines the decision-making framework of reinforcement learning with the representational power of deep learning, allowing for end-to-end learning from raw sensory input to actions. Modern DRL algorithms include Deep Q-Networks (DQN) [@mnih] for discrete action spaces, Proximal Policy Optimization (PPO) [@schulman2017] for both discrete and continuous control, and Soft Actor-Critic (SAC) [@haarnoja] for continuous control tasks. While this brings tremendous capabilities, it also introduces new challenges including sample efficiency, stability, and reproducibility that continue to be active areas of research.

A significant challenge in scientific computing is the two-language problem, where researchers must choose between high-level dynamic languages for rapid prototyping and experimentation or low-level compiled languages for performance-critical implementations. Julia is a programming language designed for scientific computing (although it is general purpose and can be used for many other tasks) that addresses this challenge by combining the ease of high-level scripting with the performance of compiled languages, making it well-suited for deep reinforcement learning applications. For the specific control tasks I am interested in—controlling fluid flow—Julia offers unique advantages. Developing and tuning RL environments requires rapid iteration and experimentation, while the optimization of neural networks and fluid simulations are both computationally intensive. Julia provides a unified language that can handle all these requirements efficiently, from high-level algorithm development to performance-critical numerical computations.

Despite Julia's suitability for DRL, the ecosystem currently lacks a comprehensive, feature-rich DRL library. Existing packages have significant limitations: Reinforcementlearning.jl was abandoned during a refactor, POMDPs.jl focuses on classical methods with only minimal DQN support, Crux.jl provides good functionality but lacks important implementation details like advantage normalization and shared network parameters, and DeepRL.jl offers only a single DQN algorithm.

Beyond addressing this gap in the Julia ecosystem, this project serves a second motivation: to deepen the author's understanding of DRL algorithms, particularly Proximal Policy Optimization (PPO). The goal is to create a prototype implementation of PPO that includes support for vectorized environments, e.g. collecting data from multiple environments in parallel, incorporates many important implementation details often overlooked in other libraries, and provides flexibility for custom neural network architectures, including the possibility of shared parameters between neural networks.

# Deep Reinforcement Learning

The core of deep reinforcement learning lies in the iterative agent-environment interaction loop. At each time step, the agent receives an observation, often also called the state, from the environment, processes this observation through a neural network to determine an action, executes that action in the environment, and receives a reward signal. This cycle continues until the environment reaches a terminal state - could be failed or successfully finished- or reaches a time limit. The agent's goal being to maximize the cumulative reward over time. An important part of training a neural network is to not always execute the action that the network believes to be best. As in real life, a good way to learn something is to try different ways, and learn from both the successes and the failures. This is called exploration. How to balance exploration and exploitation (always taking the action that the network believes to be best) is a key challenge in reinforcement learning, and there are many different approaches to this.

## Policy gradient methods

Policy gradient methods are class of algorithms that use a neural network to directly map observations to actions. This is in contrast to value-based methods, where the agent learns a value function that maps states to expected future rewards, and then uses this value function decide on what action to take.

The neural network that maps observations to actions is called a policy network, or an actor network. The design of this network depends critically on the type of action space:

**Discrete Action Spaces**: For environments with a finite set of discrete actions (e.g., move left, right, up, down), the policy network outputs logits (unnormalised probabilities) or probabilities for each possible action. The network typically has as many output neurons as there are actions, and a softmax activation function converts the logits into a valid probability distribution. Based on if the agent is exploring or exploiting, the agent either samples an action according to these probabilities, or takes the action with the highest probability.

**Continuous Action Spaces**: For environments requiring continuous control (e.g., torque values, steering angles), the policy network outputs parameters of a continuous probability distribution, most commonly a Gaussian distribution. The mean of the distribution represents the believed best action action to take, and the standard deviation controls the level of exploration, often referred to as the level of noise. As with discrete spaces, we either sample from the resulting distribution, or take the action with the highest probability. The noise can be handled in different ways, including the following:

1.  **State-dependent noise**: The policy network outputs both the mean and standard deviation of the Gaussian distribution. This allows the exploration noise to adapt based on the current state, potentially leading to more intelligent exploration strategies.

2.  **State-independent noise**: The policy network outputs only the mean of the distribution, while the standard deviation is maintained as a separate learnable parameter (or set of parameters for multi-dimensional action spaces) that is independent of the state. This approach is simpler and often more stable, especially early in training.

3.  **Separate noise network**: Some implementations use a separate neural network to predict the standard deviation, allowing for more complex state-dependent noise patterns while maintaining architectural flexibility.

The choice between these approaches affects both the exploration behavior and the complexity of the policy optimization. State-independent noise is often preferred for its simplicity and stability, while state-dependent approaches can potentially achieve more sophisticated exploration strategies. Herein we state-independent noise, when dealing with continuous action spaces.

## The Policy gradient theorem

The theoretical foundation for policy gradient methods is the policy gradient theorem, which provides an unbiased estimator for the gradient of the expected return with respect to the policy parameters. This theorem shows that the gradient can be estimated using samples from the current policy, making it practical to optimize policies through gradient descent on collected experience.

Let $\pi_\theta$ denote the policy network parameterized by $\theta$. By a trajectory we mean a sequence of tuples of state, action and reward, ${(s_i, a_i, r_i)}_{i=0}^T$ starting from state $s_0$, until the environment reaches a terminal state at time $T$. The total return from a trajectory is $R(\tau) = \sum_{t=0}^T r(s_t, a_t)$ where $T$ is the length of the trajectory and $r(s_t, a_t)$ is the reward received from taking action $a_t$ in state $s_t$. By tuning the parameters of the policy network, we want to maximise the expected return over all trajectories that is reachable from the current policy:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]
$$

We rewrite the expected return as a sum over all trajectories: $$
J(\theta) = \sum_{\tau \sim \pi_\theta} P(\tau | \theta) R(\tau),
$$ where $P(\tau | \theta)$ is the probability of a trajectory $\tau$ under the policy $\pi_\theta$. We then take the gradient (with respect to the parameters $\theta$): $$
\nabla J(\theta) = \sum_{\tau \sim \pi_\theta} \nabla P(\tau | \theta) R(\tau).
$$

Using the fact that $\nabla \log P(\tau | \theta) = \nabla P(\tau | \theta) / P(\tau | \theta)$, we can rewrite the gradient as: $$
\nabla J(\theta) = \sum_{\tau \sim \pi_\theta} P(\tau | \theta) \nabla \log P(\tau | \theta) R(\tau).
$$

This can be rewritten as the expectation of the last two factors: $$
\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} [\nabla \log P(\tau | \theta) R(\tau)].
$$ {#eq-grad-j-P}

Now we need to expand $P(\tau | \theta)$ in the expectation. We can do this by using the law of total probability: $$
P(\tau | \theta) = P(s_0) \prod_{t=0}^{T} P(s_{t+1} | s_t, a_t) \pi_\theta(a_t | s_t).
$$ If take the logarithm of both sides, we get: $$
\log P(\tau | \theta) = \log P(s_0) + \sum_{t=0}^{T} \log P(s_{t+1} | s_t, a_t) + \sum_{t=0}^{T} \log \pi_\theta(a_t | s_t)
$$ Only the last sum contains terms involving the policy parameters $\theta$. Thus we can write: $$
\nabla \log P(\tau | \theta) = \nabla \sum_{t=0}^{T} \log \pi_\theta(a_t | s_t).
$$

Inserting this into @eq-grad-j-P, we obtain

$$
\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\nabla \sum_{t=0}^{T}\log \pi_\theta(a_t | s_t) R(\tau)\right]
$$ {#eq-policy-gradient}

This equation is very useful, especially as it does not depend on any environment dynamics, only the policy and the reward signals. Using the notation $G_t = \sum_{t'=t}^T r_{t'}$, and using a few statistical tricks, we can rewrite the policy gradient as [@sutton2018]: $$
\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\nabla \sum_{t=0}^{T}\log \pi_\theta(a_t | s_t) G_t\right]
$$ {#eq-policy-gradient-2} This is the basis for policy gradient methods, and by estimating the expected value we can estimate the gradient $\nabla J(\theta)$, which we can use to update the policy parameters. Several different policy gradient methods differ in how they estimate the gradient. We will focus on the PPO algorithm, widely regarded as one of the most effective and stable policy gradient methods.

First we introduce the notion of a baseline. A baseline is a function $b(s_t)$ that is used to reduce the variance of the gradient estimate. As long as the baseline is not correlated with the action, it will not affect the gradient estimate, and we can insert it into the policy gradient equation: $$
\nabla J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[\nabla \sum_{t=0}^{T}\log \pi_\theta(a_t | s_t) (G_t - b(s_t))\right]
$$ {#eq-policy-gradient-3}

A common baseline function is the state-value function $V(s_t) = \mathbb{E}_{\tau \sim \pi_\theta} [G_t | s_t]$. In practise we dont know this value function, but we can estimate it using a neural network, called the critic or value network $V_\theta(s_t)$. The critic network is trained alongside the actor network. This approach is called actor-critic methods [@konda1999]. By subtracting this from the return, we can reduce the variance of the gradient estimate. The resulting term $G_t - V_\theta(s_t)$ is called the advantage function, and is denoted $A_t$. Since $G_t$ is the future rewards resulting from taking action $a_t$ in state $s_t$, the advantage function is signaling how much better (or worse) the action is compared to the what is expected. This leads to, following the notation of the original PPO paper [@schulman2017], to an estimated gradient at time step t: $$
g_t = \hat{\mathbb{E}}_{t} \left[\nabla \log \pi_\theta(a_t | s_t) \hat{A}_t\right],
$$ {#eq-gradient-estimator} where $\hat{\mathbb{E}}$ is an average over a batch of trajectories, and $\hat{A}_t$ is an estimate of the advantage function. When using Automatic differentiation to compute the policy updates we need to have an objective function, for which the gradient is equal to the gradient estimator in @eq-gradient-estimator:

$$
L^{PG}(\theta) = \hat{\mathbb{E}}_{t} \left[\log \pi_\theta(a_t | s_t) A_t\right].
$$ {#eq-pg-obj}

## Proximal Policy Optimization (PPO)

In the PPO algorithm, a modified objective function is used. Let $r_t(\theta)$ be the ratio of the updated policy to the old policy (before applying batched updates), that is $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$. The PPO objective function is then: $$
L^{Clip}(\theta) = \hat{\mathbb{E}}_{t} \left[\min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t)\right],
$$ {#eq-ppo-obj} where $\epsilon$ is a hyperparameter that controls the clipping (typically $\epsilon = 0.2$). If the action is good, i.e. the advantage is positive, we want to increase the probability of taking that action. If the action is bad, i.e. the advantage is negative, we want to decrease the probability of taking that action. This is achieved by the term $r_t \hat{A}_t$ when we maximize the objective function. Taking the minimum of this term and the clipped term, we ensure that the policy update is not too large. Note that when implementing this function, we multiply by a minus sign, to be consistent with optimisers that minimize.

### Generalized Advantage Estimation (GAE)

An important part of the PPO algorithm is the use of Generalized Advantage Estimation (GAE)[@schulman] to estimate the advantage function. The advantage function $A_t = G_t - V(s_t)$ measures how much better an action is compared to the average value of being in that state. However, directly computing $G_t$ requires waiting until the end of an episode, which can be inefficient and lead to high variance estimates.

GAE addresses the bias-variance trade-off by providing a family of advantage estimators that use different time horizons. The key insight is that we can estimate advantages using temporal difference (TD) errors. Define the TD error [@sutton1988] as $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$. A one-step advantage estimate is simply $\hat{A}_t^{(1)} = \delta_t$, which has low variance but may be biased if the value function is inaccurate. Alternatively, we could use the full return $G_t$, which is unbiased but has high variance.

GAE interpolates between these extremes using an exponentially-weighted average of n-step advantage estimates:

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

where $\lambda \in [0,1]$ is the GAE parameter that controls the trade-off. $\gamma$ is the discount factor, which is a hyperparameter that controls how much we value future rewards. When $\lambda = 0$, GAE reduces to the one-step estimate $\hat{A}_t^{GAE} = \delta_t$ with low variance but potential bias. When $\lambda = 1$, it approaches using the full return with high variance but no bias.

In practice, we cannot sum to infinity, so we use bootstrapping to truncate the sum at a finite horizon. Bootstrapping means using the value function estimate to approximate the remaining future returns. When an episode terminates naturally, we set the final value to zero. However, when we reach a predetermined time limit or the end of our collected trajectory, we "bootstrap" by using the value function estimate $V(s_T)$ to approximate the value of continuing from that state. This allows us to compute GAE over finite trajectories:

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{T-t-1} (\gamma \lambda)^l \delta_{t+l} + (\gamma \lambda)^{T-t} V(s_T)$$

where $T$ is either the natural episode termination or our chosen truncation point. The formula can be computed recursively as:

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \delta_t + \gamma \lambda \hat{A}_{t+1}^{GAE(\gamma,\lambda)}$$

with the boundary condition that $\hat{A}_T^{GAE} = 0$ for naturally terminated episodes, or $\hat{A}_T^{GAE} = V(s_T)$ for truncated episodes. In practice, intermediate values like $\lambda = 0.95$ provide a good balance, allowing the algorithm to learn more efficiently by reducing variance while maintaining reasonable bias levels.

### Entropy, total objective function and the final algorithm

Since PPO uses both a policy network (actor) and a value network (critic), we need to optimize the parameters of both networks. The value network is trained to minimize the mean squared error between its predictions and the actual returns or GAE targets:

$$L^{VF}(\theta) = \hat{\mathbb{E}}_{t} \left[(V_\theta(s_t) - \hat{R}_t)^2\right]$$

where $\theta$ are the network parameters and $\hat{R}_t$ is the GAE-estimated return. Specifically, the target return is computed as:

$$\hat{R}_t = \hat{A}_t^{GAE(\gamma,\lambda)} + V_{\theta_{old}}(s_t)$$

This means we use the GAE advantage estimate plus the value estimate from the previous iteration (before the current update) to create the target for training the value network. Note that $V_{\theta_{old}}(s_t)$ represents the value function from the previous parameters, while $V_\theta(s_t)$ in the loss function represents the current value function being optimized.

Additionally, to encourage exploration and prevent the policy from becoming too deterministic too early in training, an entropy bonus is often added to the objective function. Entropy measures the randomness or uncertainty in the policy's action distribution. For a continuous action space with a Gaussian policy, the entropy is:

$$H(\pi_\theta(s_t)) = \frac{1}{2} \log(2\pi e \sigma^2)$$.

where $\sigma$ is the standard deviation of the action distribution. Higher entropy means the policy is more exploratory (more random), while lower entropy means the policy is more deterministic. By adding an entropy bonus to the objective, we encourage the agent to maintain some level of exploration throughout training.

The complete PPO objective function combines all three terms:

$$L^{TOTAL}(\theta) = -L^{Clip}(\theta) + c_1 L^{VF}(\theta) - c_2 H(\pi_\theta)$$

where $c_1$ and $c_2$ control the relative importance of the value function loss and entropy bonus, respectively. Typical values are $c_1 = 0.5$ and $c_2 \in [0.0, 0.01]$. This combined objective allows us to train both the policy and value networks simultaneously using gradient-based optimization, while maintaining appropriate exploration behavior.

The full PPO algorithm is shown in @alg-ppo.

``` pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true
#| label: alg-ppo

\begin{algorithm}
\caption{Proximal Policy Optimization (PPO)}
\begin{algorithmic}
\Procedure{PPO}{$\pi_\theta, V_\theta, \text{environments}$}
  \State Initialize policy network $\pi_\theta$ and value network $V_\theta$
  \For{$\text{iteration} = 1$ \textbf{to} $N_{\text{iterations}}$}
    \State $\pi_{\theta_{\text{old}}} \leftarrow \pi_\theta$, $V_{\theta_{\text{old}}} \leftarrow V_\theta$ \Comment{Store current networks}
    \State Initialize rollout buffer $\mathcal{B} = \emptyset$
    \For{$\text{env} = 1$ \textbf{to} $N_{\text{envs}}$}
      \State perform $N_{\text{steps}}$ steps in the environment, collecting one or more trajectories $\{\tau_i\}_{i=1}^{N_{\text{trajectories}}}$
      \State Add $\{\tau_i\}_{i=1}^{N_{\text{trajectories}}}$ to rollout buffer $\mathcal{B}$
    \EndFor
    \State Compute GAE advantages $\hat{A}_t$ for all transitions in $\mathcal{B}$ using $V_{\theta_{\text{old}}}$
    \State Compute returns $\hat{R}_t = \hat{A}_t + V_{\theta_{\text{old}}}(s_t)$ for all transitions
    \For{$\text{epoch} = 1$ \textbf{to} $N_{\text{epochs}}$}
      \For{$\text{batch} = 1$ \textbf{to} $N_{\text{mini-batches}}$}
        \State Sample mini-batch $\mathcal{M}$ from rollout buffer $\mathcal{B}$
        \State Compute policy ratio $r_t = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$
        \State Compute clipped objective $L^{\text{Clip}}(\theta) = \min(r_t \hat{A}_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon) \hat{A}_t)$
        \State Compute value loss $L^{\text{VF}}(\theta) = (V_\theta(s_t) - \hat{R}_t)^2$
        \State Compute entropy bonus $H(\pi_\theta)$
        \State Compute total loss $L^{\text{TOTAL}} = -L^{\text{Clip}} + c_1 L^{\text{VF}} - c_2 H$
        \State Update $\theta$ with the chosen optimizer using the gradient of $L^{\text{Total}}$
      \EndFor
    \EndFor
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
```

## Network architectures

The choice of neural network architecture in deep reinforcement learning depends heavily on the nature of the observation space and the specific requirements of the task. Different observation types call for different architectural approaches to effectively extract relevant features and make good decisions.

**Image-based Observations**: When dealing with visual inputs such as camera feeds or rendered game screens, convolutional neural networks (CNNs) are typically employed for feature extraction. The CNN layers process the raw pixel data to extract spatial features, which are then fed into fully connected layers that form the policy and value network heads. This approach has proven highly effective in domains like Atari games and robotic vision tasks.

**Low-dimensional Continuous Observations**: For environments with relatively simple observation spaces (such as joint angles, velocities, or sensor readings), fully connected networks are often sufficient. These networks can efficiently process the structured numerical data without the spatial inductive biases of CNNs. The output architecture depends on the action space: for continuous control, the policy network outputs the mean of the action distribution, while the value network outputs a single scalar representing the state value.

**Sequential Dependencies**: Some environments require memory of past observations to make optimal decisions. In such cases, recurrent neural networks (RNNs), often combined with Long Short-Term Memory (LSTM) networks can be incorporated to maintain temporal context. This is particularly important in partially observable environments where the current observation alone is insufficient for decision-making.

**Shared vs. Separate Networks**: A key architectural decision is whether to use shared parameters between the policy and value networks. Shared networks use a common feature extraction backbone with separate heads for policy and value outputs, which can improve sample efficiency by sharing learned representations, and also speed up computations when outputs from both networks are needed. Alternatively, completely separate networks provide more flexibility but require more parameters and potentially more training data.

A major goal of our implementation is to support arbitrary network architectures, allowing researchers to experiment with custom designs tailored to their specific domains.

# Implementation

The implementation of our prototype DRL library DRiL.jl [@holme2025] follows a modular design philosophy that separates concerns into four main components: environments, policies, agents, and algorithms. This separation allows for flexible composition and easy extension of the library.

**Environments** handle the interaction dynamics and provide the problem specification through observation and action spaces. **Policies** encapsulate the neural network architectures, including both policy and value networks, along with any associated noise parameters or other architectural components. **Agents** serve as the orchestration layer, managing policy parameters, optimization procedures, logging, and the overall training loop. Finally, **algorithms** define the specific loss functions, hyperparameters, and algorithmic details that distinguish different DRL methods.

This modular approach enables researchers to mix and match components—for instance, using the same policy architecture with different algorithms, or applying the same algorithm to different environment types. The clear separation also facilitates testing, debugging, and extension of individual components without affecting the entire system.

## Environments

The environment interface is designed to support both single and parallel environments while maintaining flexibility for different use cases. Our design draws inspiration from both the the interface from the CommonRLInterface.jl package and the Gymnasium [@gymnasiu] (a widely used library for RL environments) interface, combining their strengths to create a more comprehensive solution.

### Interface Design

The environment interface is built around two main abstract types: `AbstractEnv` for single environments and `AbstractParallellEnv` for parallel environments. This hierarchy allows for specialized implementations while maintaining a common interface for basic operations.

For single environments, we extend the interface from CommonRLInterface.jl with additional functionality to handle episode truncation correctly. The CommonRL interface provides a clean, minimalist approach that deviates from the standard Gymnasium interface but offers greater flexibility for custom implementations. Key functions include `reset!(env)` to initialize/reset episodes, `act!(env, action)` to execute actions and receive rewards, `observe(env)` to get current observations, and separate `terminated(env)` and `truncated(env)` functions to distinguish between natural episode endings and time-based cutoffs.

The distinction between termination and truncation is crucial for proper bootstrapping in value function estimation. When an episode terminates naturally (e.g., the agent reaches a goal or fails), the value of the final state should be zero. However, when an episode is truncated due to time limits, we need to bootstrap using the value function estimate to approximate the remaining return, as discussed previously.

For parallel environments, we adopt the step interface pattern from Gymnasium, which assumes automatic sub-environment resetting and comprehensive information passing. The `step!(env, actions)` function returns rewards, termination flags, truncation flags, and additional information for each environment. When individual environments are truncated, the final observation before reset is provided in the info dictionary as `"terminal_observation"`, enabling proper bootstrapping calculations.

While using both interface patterns adds complexity compared to adopting a single approach, this dual design provides significant benefits. The lower-level interface from CommonRLInterface.jl allows for maximum flexibility when implementing custom parallel environments or specialized interaction patterns. Meanwhile, the higher-level step interface with automatic resetting simplifies the implementation of standard parallel training loops.

### Multithreaded Parallel Environments

Our implementation utilize the natively multithreaded nature of Julia, allowing multiple environment instances to run concurrently on different CPU threads. This parallelization significantly improves sample collection efficiency, especially for computationally lightweight environments. The parallel environment abstraction handles thread safety and synchronization automatically, presenting a simple interface to the training algorithm.

The current multithreaded implementation could be extended to support distributed environments across multiple processes or even multiple machines, enabling scalable training for more computationally demanding environments or larger-scale experiments. This is not implemented in the current version of the package, but could be implemented in the future.

## Spaces

Action and observation spaces define the structure and constraints of the environment's inputs and outputs. Our implementation provides a foundation for various space types, with particular focus on continuous control tasks.

The space system is built around an abstract type hierarchy starting with `AbstractSpace`, which allows for extensible space definitions. For continuous control tasks, the most important space type is the Box space, which represents multi-dimensional continuous spaces with defined bounds (although the bounds may be infinite). Another commonly used space type is the Discrete space, which represents a finite set of discrete values. We implement both the Box and the Discrete spaces. The `Box{T}` type is parameterized by the numeric type `T` (typically `Float32`) and contains three key components: `low` and `high` bounds that specify bounds for all dimensions, and a `shape` tuple that defines the dimensionality.

The `Discrete` type is parameterized simpler than the Box type, and contains a `n` field, as well as a 'start' field that defines the starting value.

The space implementation integrates seamlessly with Julia's random number generation system and Multiple Dispatch architecture by extending `Random.rand()` for our spaces. This allows for easy sampling of valid actions during exploration or testing. The sampling function generates uniform random values space bounds and can handle both single samples and batch sampling for vectorized environments. Additionally, the `in` operator is overloaded to provide efficient bounds checking, ensuring that actions or observations fall within the valid space constraints.

## Neural Networks

Our neural network implementation leverages Lux.jl [@pal2025], a modern Julia package for scientific machine learning that provides several advantages over traditional deep learning frameworks. Lux.jl is a pure Julia implementation with explicit parameterization, meaning that network parameters are separate from the network structure itself. This design enables more flexible parameter manipulation, easier debugging, and better integration with Julia's scientific computing ecosystem.

### Lux.jl Framework Benefits

The choice of Lux.jl brings several technical advantages. The explicit parameterization allows for clean separation between network architecture and learned parameters, facilitating advanced techniques like parameter sharing, meta-learning, and custom optimization schemes. The framework supports custom weight initialization through WeightInitializers.jl, enabling implementation of specific initialization strategies like orthogonal initialization that are common in RL training.

Lux.jl's support for multiple automatic differentiation backends provides flexibility and performance. Furthermore, the framework's compatibility with Reactant.jl enables acceleration on GPUs and even TPUs through compilation to MLIR and XLA, providing a path to high-performance training when needed.

### Policy Architecture

The policy implementation follows an actor-critic architecture through the `AbstractActorCriticPolicy` type, which encapsulates both policy and value networks. We have implemented one policy type ('DiscreteActorCriticPolicy') for Discrete action spaces and one for Continuous action spaces ('ContinuousActorCriticPolicy'). The architecture consists of three main components: a feature extractor that processes raw observations, an actor head that outputs action distributions, and a critic head that estimates state values.

For continuous control tasks, the default architecture uses fully connected layers with orthogonal weight initialization. The feature extractor flattens the input observations, while the actor and critic heads consist of two hidden layers with configurable activation functions (defaulting to `tanh`). The actor head outputs the mean of the action distribution, while a separate learnable parameter maintains the log standard deviation for exploration noise (For continuous action spaces).

The implementation supports both shared and separate network architectures. In the shared approach, the feature extractor parameters are common to both actor and critic, with separate heads for policy and value outputs. This parameter sharing can improve sample efficiency by allowing the networks to learn common representations. Alternatively, completely separate networks can be used when more architectural flexibility is required. The default is to use separate networks for continuous tasks, and we follow that practice herein.

### Policy Interface

The policy interface provides a clean abstraction for different types of neural network policies. Key methods include `predict()` for action selection (with optional deterministic mode), `predict_values()` for value estimation, and `evaluate_actions()` for computing log probabilities and entropy of given state-action pairs. The main call operator of the policies combines action prediction and value estimation in a single forward pass, which is essential for efficient actor-critic training.

The interface handles the complexities of continuous action spaces, including proper distribution parameterization, reparameterization trick for differentiable sampling, and action clipping to respect environment bounds.

### Unified Loss Function

Unlike some other RL libraries that train actor and critic networks separately, our implementation uses a single combined loss function for all parameters. This approach simplifies the training loop and ensures that both policy and value networks are updated simultaneously with consistent gradients. The combined loss includes the PPO clipped objective for the policy, mean squared error for value function learning, and an entropy bonus for exploration, all weighted by appropriate coefficients and optimized together using a single optimizer instance.

## Agents

The agent serves as the central orchestration component that manages the policy, parameters, optimization, and logging throughout the training process. In our modular design, the agent acts as the interface between the high-level training algorithm and the lower-level policy and environment components.

### Agent Architecture

The `ActorCriticAgent` struct encapsulates all the necessary components for training actor-critic policies. It maintains a reference to the policy network, a training state that includes parameters and optimizer state, and various hyperparameters that control the training process. The agent also manages training statistics and optional logging functionality.

The training state is built using Lux.jl's `TrainState`, which provides a clean abstraction for managing network parameters, internal states (used by LSTM layers and other layer types), and optimizer state in a unified structure. This design ensures that all components remain synchronized during training and simplifies parameter updates and state management.

Key hyperparameters managed by the agent include the number of steps taken in each environment per rollout (`n_steps`), mini-batch size for gradient updates (`batch_size`), number of training epochs per rollout (`epochs`), learning rate, and optimizer type. These parameters can be configured during agent construction, allowing for easy hyperparameter experimentation.

### Training Interface

The agent provides a clean interface for the core operations needed during training. The `get_action_and_values()` method performs a complete forward pass through the policy, returning actions, value estimates, and log probabilities simultaneously. For inference and evaluation, the agent provides separate methods: `predict_actions()` for action selection (with optional deterministic mode) and `predict_values()` for value estimation. These methods handle the underlying parameter and state management automatically, presenting a simple interface to the calling code.

The agent automatically manages the internal states of the neural networks, ensuring that any layers that maintain state (such as recurrent layers) maintain their state correctly across function calls. This state management is handled transparently through Lux.jl's state system.

### Statistics and Logging

Training statistics are tracked through an `AgentStats` structure that monitors key metrics such as the number of gradient updates performed and total steps taken in the environment. These statistics are essential for monitoring training progress and can be used for learning rate scheduling or other adaptive training strategies.

The agent integrates with TensorBoardLogger.jl to provide comprehensive logging capabilities. When a log directory is specified during agent construction, training metrics, loss values, and other relevant statistics are automatically logged to TensorBoard format [@developers2025]. This logging functionality is particularly valuable during development of both the library and new environments, as it provides real-time insights into training dynamics and helps identify potential issues.

The verbosity level can be controlled to show progress bars and additional statistics during training, providing flexibility for different use cases.

### Randomness

Random number generation is ubiquitous throughout reinforcement learning systems, appearing in multiple critical components: random initialization of environment states, random initialization of neural network parameters, random sampling from action distributions during exploration, and random sampling/shuffling of mini-batches during training. The proper handling of random number generators (RNGs) is crucial for ensuring reproducibility, debugging, and fair comparison of different algorithms and hyperparameters.

Reproducibility in deep reinforcement learning is particularly challenging due to the stochastic nature of the learning process. Small differences in random seeds can lead to dramatically different learning trajectories, making it essential to have precise control over all sources of randomness. Without careful RNG management, it becomes impossible to reproduce experimental results, debug training issues, or conduct meaningful ablation studies.

Our implementation uses Julia's Random.jl standard library to handle all random number generation in a principled manner. Random.jl provides a robust framework for managing multiple independent random number generators, each with their own state and seed. This allows us to isolate different sources of randomness and control them independently, which is essential for both reproducibility and parallel execution.

We extend the standard `rand()` and `seed!()` functions to work seamlessly with our custom space and environment types. For example, calling `rand(action_space)` automatically samples a valid action from the specified action space bounds, while `seed!(env, seed_value)` sets the random seed for a specific environment instance. This extension provides a clean, intuitive interface that integrates naturally with Julia's existing random number ecosystem.

The architecture employs a distributed RNG approach where each major component maintains its own independent random number generator. Each environment instance has its own RNG, which is used for environment-specific randomness such as initial state sampling, stochastic dynamics, and any environment-internal random events. This design ensures that the randomness in one environment does not affect others, which is crucial for parallel environment execution and reproducible results across different numbers of parallel workers.

Similarly, each agent maintains its own RNG that is used for policy-related randomness, including sampling from action distributions during both training and inference, for initializing neural network parameters, and for shuffling the rollout buffer. By separating the agent's RNG from the environment RNGs, we ensure that changes in environment behavior (such as adding more parallel environments) do not affect the policy's random decisions, maintaining consistency in the learning process.

This careful separation of random number generators provides several important benefits. First, it enables perfect reproducibility—given the same initial seeds, the entire training process will produce identical results regardless of the execution environment. Second, it facilitates debugging by allowing researchers to isolate sources of randomness and test specific scenarios. Third, it supports fair hyperparameter comparisons by ensuring that different hyperparameter settings are evaluated under identical random conditions.

## PPO Implementation

Proximal Policy Optimization is notoriously difficult to reproduce accurately, with different implementations employing subtly different implementation details that can significantly impact performance. As noted in the comprehensive blog post by [Huang et al. (2022)](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) [@the37i], mainly covering aspects from two papers, [@engstrom2019; @andrychowicz], there are 37 distinct implementation details that can affect PPO's behavior, and the choice of which details to include can make the difference between successful learning and complete failure.

The challenge stems from the fact that the original PPO paper, while providing the core algorithmic framework, does not specify many crucial implementation details that are necessary for practical success. Different research groups and libraries have made different choices for these details, leading to a proliferation of PPO variants that can produce vastly different results even when applied to the same environment.

To ensure reproducibility and transparency, we explicitly document our implementation choices by following the recommendations from the above blog post. Below, we detail which implementation choices we have made and provide justification for each decision, particularly focusing on the details most relevant to continuous control tasks.

### Core Implementation Details

**1. Vectorized Environments**: ✅ **Implemented** We use vectorized environments to collect experience from multiple environment instances in parallel. This significantly improves sample collection efficiency and is essential for stable PPO training. Our implementation currently only supports multithreaded environment execution.

**2. Orthogonal Weight Initialization and constant bias initialization**: ✅ **Implemented**\
We initialize network weights using orthogonal initialization [@saxe]with specific gain values: √2 for hidden layers, 0.01 for the actor output layer, and 1.0 for the critic output layer. This initialization strategy helps maintain appropriate gradient magnitudes during early training and is crucial for stable learning in continuous control tasks.

**3. Adam Optimizer with** $\epsilon=1e-5$ : ✅ **Implemented**\
Following the original OpenAI implementation, we use the Adam optimizer [@kingma] with ε=1e-5 rather than the default ε=1e-8. While this detail may seem minor, it can affect numerical stability, particularly in environments with small reward scales.

**4. Learning Rate Annealing**: ❌ **Not Implemented**\
We do not implement learning rate annealing, as our experiments with the Pendulum environment showed that constant learning rates were sufficient for successful learning. This simplification reduces hyperparameter complexity without apparent performance loss for our target tasks.

**5. Generalized Advantage Estimation (GAE)**: ✅ **Implemented**\
We implement GAE with proper bootstrapping for truncated episodes. Importantly, our implementation correctly distinguishes between terminated and truncated episodes—when an episode is truncated due to time limits, we bootstrap using the value function estimate, whereas naturally terminated episodes use zero as the final value.

**6. Mini-batch Updates**: ✅ **Implemented**\
The collected rollout buffer is divided into mini-batches for gradient updates. This allows for multiple gradient steps per rollout while maintaining computational efficiency and helping to stabilize training.

**7. Advantage Normalization**: ✅ **Implemented**\
We normalize advantages across each mini-batch to have zero mean and unit variance. This normalization helps stabilize training by ensuring that the advantage estimates have consistent scale across different episodes and environments.

**8. Clipped PPO Objective**: ✅ **Implemented**\
We implement the core clipped surrogate objective that defines PPO, using the default clipping parameter ε=0.2. This prevents excessively large policy updates that could destabilize training.

**9. Value Function Loss Clipping**: ❌ **Not Implemented**\
We do not implement value function loss clipping, as recent studies suggest it is either unnecessary or potentially harmful for performance. Our implementation uses the standard mean squared error loss for value function training.

**10. Combined Loss Function**: ✅ **Implemented**\
We use a single combined loss function that includes the clipped policy objective, value function loss, and entropy bonus. This unified approach ensures consistent gradient updates across all network components.

**11. Global Gradient Clipping**: ✅ **Implemented**\
We implement global gradient clipping with a maximum norm of 0.5, as recommended in the original PPO paper. This prevents gradient explosion and helps maintain training stability.

**12. Debug variables**: ✅ **Implemented**\
We implement debug variables to monitor the training process. These variables are not used in the training loop, but are logged to TensorBoard for monitoring.These variables are: policy loss, value loss, entropy loss, clip fraction (the fraction of loss function terms that were clipped), approximate KL divergence (a measure of how much the policy has changed).

**13. Network Architecture**: ✅ **Implemented**\
We support both shared and separate network architectures. For continuous control tasks, we default to separate networks for actor and critic, which is the standard practice for such environments and provides architectural flexibility.

### Continuous Action Space Details

**1. Gaussian Action Distributions**: ✅ **Implemented**\
Our policy network outputs the mean of a Gaussian distribution for continuous actions. The standard deviation is maintained as a separate learnable parameter, enabling proper exploration in continuous action spaces.

**2. State-Independent Standard Deviation**: ✅ **Implemented**\
We use state-independent noise with learnable log standard deviation parameters for each action dimension. This approach is simpler and often more stable than state-dependent noise, particularly early in training.

**4. Separate policy and value networks**: ✅ **Implemented**\
Although we support parameter sharing, we default to separate networks for continuous control tasks.

**5. Action Clipping**: ✅ **Implemented**\
Actions are clipped to the valid action space bounds before being sent to the environment. However, we store the original (unclipped) actions in the rollout buffer to maintain proper gradient flow during policy updates.

**6.-9. Observation and Reward Normalization**: ✅ **Implemented**\
We implement observation and reward normalization through the environment wrapper type 'NormalizeWrapperEnv'.

### Auxiliary implementation details

**3. Early Stopping of Policy Updates**: ✅ **Implemented**\
We implement early stopping of policy optimization based on the approximate KL divergence between the old and new policies. If this divergence exceeds a specified threshold during training, we halt the current policy update phase and return to collecting new rollouts. This helps prevent the policy from changing too drastically in a single update, maintaining training stability.

This careful documentation of implementation details serves multiple purposes: it ensures reproducibility of our results, provides transparency about our design choices, and offers guidance for researchers who wish to extend or modify our implementation for their own work.

# Results

To validate our PPO implementation and demonstrate its effectiveness, we evaluate the algorithm on the classical Pendulum environment and the continuous MountainCar environment[^2]. The environments serve as an excellent testbed for continuous control algorithms and provide a clear benchmark for assessing whether our implementation can successfully learn meaningful policies.

[^2]: Implemented here: <https://github.com/KristianHolme/ClassicControlEnvironments.jl>

## The Pendulum Environment

The Pendulum environment is part of the Classical Control suite of environments available in Gymnasium, representing one of the fundamental benchmarks for continuous control algorithms. The task involves controlling an inverted pendulum to remain upright by applying torque at its base—a classic problem in control theory that has been extensively studied in both traditional control and reinforcement learning contexts.

### Environment Dynamics

The environment simulates a pendulum of fixed length swinging in a vertical plane. The pendulum's state is characterized by its angle θ (measured from the upright position) and angular velocity θ̇. The angle θ ranges between -π and π radians, where θ = 0 corresponds to the pendulum pointing straight up (the desired target state), and θ = ±π corresponds to the pendulum hanging straight down.

The agent observes the environment through a three-dimensional observation vector consisting of cos(θ), sin(θ), and the angular velocity θ̇. This trigonometric representation ensures that the observation space is continuous and bounded, with cos(θ) and sin(θ) both lying in the range \[-1, 1\]. The angular velocity is bounded to the range \[-8, 8\] rad/s and is normalized to \[-1, 1\] for the agent's observation. This representation avoids the discontinuity that would arise from directly observing the angle θ, which wraps around at ±π.

The action space consists of a single continuous value representing the torque applied to the pendulum base, bounded in the range \[-2, 2\] Newton-meters. This continuous action space makes the Pendulum environment particularly suitable for testing policy gradient algorithms like PPO, which naturally handle continuous control problems.

@fig-pendulum shows a simple illustration of the Pendulum environment.

![Pendulum environment](plots/pendulum/pendulum_best_config_1.png){#fig-pendulum}

### Reward Structure

The reward function is designed to encourage the agent to keep the pendulum upright while minimizing energy consumption:

$$r_t = -\theta^2 - 0.1 \cdot \dot{\theta}^2 - 0.001 \cdot u^2$$

where θ is the angle from vertical, θ̇ is the angular velocity, and u is the applied torque. This reward structure has several important characteristics:

-   The primary term (-θ²) heavily penalizes deviations from the upright position, providing the strongest learning signal
-   The velocity term (-0.1·θ̇²) encourages smooth control by penalizing rapid movements
-   The torque term (-0.001·u²) promotes energy efficiency by discouraging unnecessarily large control actions

The maximum possible reward per timestep is 0 (achieved when the pendulum is perfectly upright and stationary with no applied torque), while the minimum reward is bounded below by approximately $-16$. According to OpenAI wiki [@leaderbo], it is not clear when this environment is solved, but a average episodic return above $-200$ is considered satisfactory.

### Episode Structure

Each episode runs for exactly 200 timesteps with a simulation timestep of 0.05 seconds, corresponding to 10 seconds of simulated time. Episodes do not terminate early based on the pendulum's state—the agent must maintain control for the full duration regardless of performance. This fixed-length episode structure ensures consistent training data and prevents the agent from learning to exploit early termination conditions.

The environment is reset to a random initial state at the beginning of each episode, with the initial angle sampled uniformly from \[-π, π\] and the initial angular velocity sampled from a smaller range around zero. This randomization ensures that the agent learns a robust policy that can handle various starting conditions rather than memorizing a solution for a specific initial state.

A well-implemented PPO algorithm should achieve average episode returns of approximately -200 to -150 after sufficient training, representing successful pendulum control with only minor deviations from the upright position. Random policies typically achieve returns around -1600, providing a clear baseline for measuring learning progress.

## Hyperparameter Search for Pendulum

To validate our PPO implementation and identify optimal hyperparameters for the Pendulum environment, we conducted a comprehensive hyperparameter search leveraging Julia's native multithreading capabilities. This search process demonstrates both the effectiveness of our implementation and the computational advantages of Julia for parallel experimentation. By combining multithreaded execution with our parallel environment implementation, we can efficiently run hundreds of environment instances simultaneously on a single machine. While the Pendulum environment is computationally lightweight, this approach showcases the scalability potential for more demanding environments in future work.

The search process samples hyperparameters from predefined ranges rather than using a fixed grid, allowing for more efficient exploration of the hyperparameter space. Key hyperparameters include the discount factor γ (0.92-0.999), GAE parameter λ (0.8-0.98), PPO clipping range ε (0.1-0.3), learning rate (1e-5 to 1e-2 on a log scale), batch size (64 or 128), steps per environment per rollout (128, 256, or 512 steps), training epochs (10 or 20), and number of parallel environments (4, 8, or 16). We also implement a scaling wrapper that scales the observations in any finite range to the range \[-1, 1\], and likewise for actions. We include this scaling wrapper in the search.

We train each hyperparameter configuration with five different random seeds. This approach ensures that our results are statistically robust and not merely the result of fortunate random initialization. The evaluation metric is the average return over multiple test episodes after training completion.

The search infrastructure uses DrWatson.jl [@datseris2020] for experiment management and result caching, ensuring reproducibility and efficient resource utilization. Each trial is automatically logged with TensorBoard for detailed monitoring of training dynamics, and results are systematically collected and analyzed to identify the best-performing configurations. The implementation tracks both individual runs and aggregated statistics across seeds.

## Training statistics using the best hyperparameters

The following hyperparameters were found to be the best:

| Hyperparameter                    | Value       |
|-----------------------------------|-------------|
| Discount factor (γ)               | 0.990886    |
| GAE parameter (λ)                 | 0.85821     |
| PPO clipping range (ε)            | 0.132141    |
| Learning rate                     | 0.000195409 |
| Batch size                        | 128         |
| Steps per environment per rollout | 128         |
| Training epochs                   | 20          |
| Number of parallel environments   | 8           |
| value coefficient (c1)            | 0.480177    |
| entropy coefficient (c2)          | 0.00999     |
| Normalization wrapper             | yes         |
| scaling wrapper                   | yes         |

: Best hyperparameters for Pendulum

Using these hyperparameters, the average episodic return across the five different seeds, each trained agent running for 100 episodes, is $-142.74796$, comfortably within the range of $-200$ to $-150$ that is considered satisfactory and better than the 8th place at the OpenAI leaderboard (although this leaderboard is very old and likely not up to date).

@fig-pendulum-best-config shows some snapshots from a trajectory obtained from an agent trained with the best hyperparameters.

::: {#fig-pendulum-best-config layout-ncol="2"}
![Step 20](plots/pendulum/pendulum_best_config_20.png){#fig-pendulum-step20}

![Step 30](plots/pendulum/pendulum_best_config_30.png){#fig-pendulum-step30}

![Step 40](plots/pendulum/pendulum_best_config_40.png){#fig-pendulum-step40}

![Step 50](plots/pendulum/pendulum_best_config_50.png){#fig-pendulum-step50}

Pendulum trajectory snapshots at different time steps during the best configuration's performance.
:::

## The Continuous MountainCar Environment

The continuous MountainCar environment is another environment from the Classical Control suite of environments available in Gymnasium, representing another benchmark for continuous control algorithms. The task involves controlling a car (through positive or negative acceleration) to reach the top of a mountain. The car is subject to a friction force, and the goal is to reach the top of the mountain in as few timesteps as possible.

@fig-mountaincar shows a simple illustration of the MountainCar environment.

![MountainCar environment](plots/mountaincar/mountaincar_best_config_1.png){#fig-mountaincar}

### Reward structure

In contrast to the Pendulum environment, the MountainCar environment has a less continuous reward structure. We want to reach the top of the mountain in as few steps as possible, so we give a small negative reward for each step taken ($-0.1*action^2$), and a huge reward (+100)for reaching the top. The OpenAi wiki states that the environment is solved when the average episodic return over 100 episodes is greater than or equal to 90.

### Hyperparameter search for MountainCar

We follow a similar approach to the hyperparameter search as for the Pendulum environment. We also include searching over the inital value of the 'log_std' parameter of the policy, the standard deviation of the action distribution.

We find that the best hyperparameters are:

| Hyperparameter                    | Value       |
|-----------------------------------|-------------|
| Discount factor (γ)               | 0.994528    |
| GAE parameter (λ)                 | 0.871101    |
| PPO clipping range (ε)            | 0.273062    |
| Learning rate                     | 1.493117e-5 |
| Batch size                        | 64          |
| Steps per environment per rollout | 512         |
| Training epochs                   | 5           |
| Number of parallel environments   | 16          |
| value coefficient (c1)            | 0.260734    |
| entropy coefficient (c2)          | 0.008623    |
| Normalization wrapper             | yes         |
| scaling wrapper                   | no          |
| inital value of log_std           | -0.145555   |

: Best hyperparameters for MountainCar

Using these hyperparameters, the average episodic return across the five different seeds, each trained agent running for 100 episodes, is $97.01644$, which is well above the threshold of 90, signaling that the agent has learned to solve the environment.

@fig-mountaincar-best-config shows some snapshots from a trajectory obtained from an agent trained with the best hyperparameters.

::: {#fig-mountaincar-best-config layout-ncol="2"}
![Step 192](plots/mountaincar/mountaincar_best_config_192.png){#fig-mountaincar-step19}

![Step 309](plots/mountaincar/mountaincar_best_config_309.png){#fig-mountaincar-step30}

![Step 372](plots/mountaincar/mountaincar_best_config_372.png){#fig-mountaincar-step37}

MountainCar trajectory snapshots at different time steps during the best configuration's performance.
:::

# Conclusion

This project has successfully achieved its primary objectives of implementing a functional PPO algorithm in Julia and demonstrating its effectiveness on classical control tasks. The prototype DRL library represents a solid foundation for future development.

The modular architecture we developed—separating environments, policies, agents, and algorithms—provides the flexibility needed for research applications while maintaining clean interfaces that facilitate debugging and extension. Our careful attention to implementation details, particularly the several of the 37 nuanced choices documented in the PPO literature [@engstrom2019; @andrychowicz], ensures that our results are reproducible and that the library, with further development, can hopefully reach a stage where it serves as a reliable baseline for future work.

The experimental validation on both Pendulum and MountainCar environments demonstrates that our implementation can successfully learn meaningful control policies. Achieving average returns of -142.75 on Pendulum and 97.02 on MountainCar places our results well within the range of successful implementations, confirming that the core algorithmic components are functioning correctly. The comprehensive hyperparameter searches not only validated our implementation but also provided insights into the sensitivity of PPO to different configuration choices in continuous control settings.

Perhaps most importantly, this work has deepened the author's understanding of the practical challenges in implementing deep reinforcement learning algorithms. Implementing this project has been a great learning experience, and the author is now much more confident in their ability to implement and debug deep reinforcement learning algorithms.

Looking forward, the foundation established here provides multiple avenues for expansion. The modular design will facilitate the addition of new algorithms, environment types, neural network architectures, and support for more performant AD backends and GPU acceleration.

While significant work remains to create a fully-featured DRL library comparable to established Python frameworks, this prototype demonstrates that Julia's unique combination of performance and expressiveness makes it an excellent choice for reinforcement learning research. The groundwork laid here in DRiL.jl [@holme2025] will support continued development toward the ultimate goal of enabling efficient DRL research for complex applications like active flow control.

# References

::: {#refs}
:::